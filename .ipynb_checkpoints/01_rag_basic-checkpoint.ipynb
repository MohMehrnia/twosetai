{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b91969-3f43-4067-857a-c02ccf857e7c",
   "metadata": {},
   "source": [
    "# YouTube Channel\n",
    "\n",
    "**Link:** https://www.youtube.com/@TwoSetAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c9c69-cf8c-4b4c-80bc-b4089a93c8c1",
   "metadata": {},
   "source": [
    "# Demystfying Retrieval Augmented Generation (RAG) Approach\n",
    "\n",
    "**What is RAG?** RAG combines the strengths of generative AI with retrieval techniques to enhance the quality and relevance of generated text.\n",
    "\n",
    "## RAG Architecture\n",
    "<img src=\"https://mallahyari.github.io/rag-ebook/diagrams/rag_architecture.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e4c2e-5e4c-4495-9d53-489a158fb69e",
   "metadata": {},
   "source": [
    "## Data Ingestion Pipeline\n",
    "\n",
    "<img src=\"https://mallahyari.github.io/rag-ebook/diagrams/rag_data_pipeline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce21dd-8d5c-4073-a736-bdb282d3c9d0",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a19c42a-d850-4d48-94c8-eed079ef7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain-text-splitters\n",
    "# pip install qdrant-client\n",
    "# pip install pypdf\n",
    "\n",
    "# OPENAI_API_KEY = \"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b408e929-ad7d-438d-acec-ae34f400333a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assisting in Writing Wikipedia-like Articles From Scratch\\nwith Large Language Models\\nYijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu\\nOmar Khattab Monica S. Lam\\nStanford University\\n{shaoyj, yuchen'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "FILE_PATH = os.path.join(\"data\",\"storm_paper.pdf\")\n",
    "reader = PdfReader(FILE_PATH)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "entire_text = \"\"\n",
    "for page_num in range(number_of_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    entire_text += page.extract_text()\n",
    "\n",
    "entire_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f183ff-ad55-4b74-9962-33c496667dd0",
   "metadata": {},
   "source": [
    "## 1. Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1334d2c-5441-410d-98aa-170d3dda5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd008b-d9b6-4f48-a145-087692556f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 215\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_splitter.split_text(entire_text)\n",
    "print(f\"Total chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654cbcd7-f7b1-4104-a38f-e15fa0029712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assisting in Writing Wikipedia-like Articles From Scratch\\nwith Large Language Models\\nYijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu\\nOmar Khattab Monica S. Lam\\nStanford University\\n{shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu\\nlam@cs.stanford.edu\\nAbstract\\nWe study how to apply large language models\\nto write grounded and organized long-form ar-\\nticles from scratch, with comparable breadth\\nand depth to Wikipedia pages. This underex-\\nplored problem poses new challenges at the',\n",
       " 'plored problem poses new challenges at the\\npre-writing stage, including how to research\\nthe topic and prepare an outline prior to writ-\\ning. We propose STORM , a writing system\\nfor the Synthesis of Topic Outlines through\\nRetrieval and Multi-perspective Question Ask-\\ning. STORM models the pre-writing stage by\\n(1) discovering diverse perspectives in research-\\ning the given topic, (2) simulating conversa-\\ntions where writers carrying different perspec-']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2276ce-56d6-43db-a965-dd1af9129a36",
   "metadata": {},
   "source": [
    "## LlamaIndex Split by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf97fd8-4aff-4174-99c7-68d36aa9e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "llamaindex_splitter = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "llamaindex_text_chunks = llamaindex_splitter.split_text(entire_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4633d6e-d784-45d3-b189-3c6902587af7",
   "metadata": {},
   "source": [
    "## 2. Embedding Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28cd1434-16a2-4cc4-a2a5-df3c1fe78d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b43824d-103b-443d-828d-3a3de52ba5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e4967b28a24f7394d3e5371d43ef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = embedding_model.encode(text_chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61efe911-e8b8-4fd3-b26e-85790c73f463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ababb2bb-6778-41b4-8180-f1ad8cbe0e09",
   "metadata": {},
   "source": [
    "## 3. Store in the Vector Database\n",
    "\n",
    "> We use Qdrant. Please see [here](https://github.com/qdrant/qdrant) for documentation.\n",
    "\n",
    "### How to run qdrant docker\n",
    "\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -p 6333:6333 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "    qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd33ccba-0673-4425-b841-b0d187989ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-client\n",
    "\n",
    "# Import client library\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fa1ff7-dabe-470b-80ee-c7d5459dcce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_model.get_sentence_embedding_dimension()\n",
    "collection_name = \"qa_index\"\n",
    "client.delete_collection(collection_name)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2971f-385e-4f1e-9ba2-153f80e50216",
   "metadata": {},
   "source": [
    "### Create payloads and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c177bc1-9ca7-4f35-88eb-b9fefb45dc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/storm_paper.pdf',\n",
       " 'content': 'Assisting in Writing Wikipedia-like Articles From Scratch\\nwith Large Language Models\\nYijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu\\nOmar Khattab Monica S. Lam\\nStanford University\\n{shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu\\nlam@cs.stanford.edu\\nAbstract\\nWe study how to apply large language models\\nto write grounded and organized long-form ar-\\nticles from scratch, with comparable breadth\\nand depth to Wikipedia pages. This underex-\\nplored problem poses new challenges at the'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = []\n",
    "payload = []\n",
    "\n",
    "for id, text in enumerate(text_chunks):\n",
    "    ids.append(id)\n",
    "    payload.append({\"source\": FILE_PATH, \"content\": text})\n",
    "\n",
    "payload[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e47de0a-e3c1-44ef-a78f-59d33e4e7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors=embeddings,\n",
    "    payload=payload,\n",
    "    ids=ids,\n",
    "    batch_size=256,  # How many vectors will be uploaded in a single request?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8111478-2347-444c-9695-43465d407bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=215)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fcbe8-aeb3-4347-9279-3aa62640f553",
   "metadata": {},
   "source": [
    "## Recap\n",
    "1. Read the pdf file and extract text\n",
    "2. Split/Chunk the textual content\n",
    "3. Embed the chunks\n",
    "4. Store the embeddings and matadata in Qdrant vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc38790-fb09-4a68-bbfb-5a6b635d549e",
   "metadata": {},
   "source": [
    "## Embedding and Storing using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "795ff7bb-f50e-4941-8724-b407ac3a8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-community\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    )]\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    path=\"/tmp/local_qdrant_storage\"\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e09c3-dab8-47a9-8a5a-420468d3f8f4",
   "metadata": {},
   "source": [
    "## Retrieval Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523a12ff-9f9d-4847-b66d-b3f05dd2693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(text: str, top_k: int):\n",
    "    query_embedding = embedding_model.encode(text).tolist()\n",
    "    \n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=None,  \n",
    "        limit=top_k\n",
    "    )\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3ceac0a-a39e-4e49-8c8d-cdad2d0c1e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=130, version=0, score=0.72634983, payload={'content': 'In §3, we introduce STORM, a framework that au-\\ntomates the pre-writing stage by discovering differ-\\nent perspectives, simulating information-seeking\\nconversations, and creating a comprehensive out-\\nline. Algorithm 1 displays the skeleton of STORM.\\nWe implement STORM with zero-shot prompt-\\ning using the DSPy framework (Khattab et al.,\\n2023). Listing 1 and 2 show the prompts used\\nin our implementation. We highlight that STORM\\noffers a general framework designed to assist the', 'source': 'data/storm_paper.pdf'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id=164, version=0, score=0.6753985, payload={'content': '50$ for our study.for each article. Figure 7 shows the screenshot of\\nour web application and the full article produced\\nby STORM is included in Table 12. For human\\nevaluation, we use a 1 to 7 scale for more fine-\\ngrained evaluation. The grading rubric is included\\nin Table 10.\\nWe collected the pairwise preferences and the\\nperceived usefulness of STORM via an online ques-\\ntionnaire. Specifically, for the perceived usefulness,\\nwe request editors to rate their agreement with state-', 'source': 'data/storm_paper.pdf'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id=167, version=0, score=0.67447704, payload={'content': 'STORM currently retrieves grounding sources\\nfrom the Internet which is not neutral and con-\\ntains considerable promotional content on its own.\\nAddressing this bias in the pre-writing stage repre-\\nsents a valuable direction for future research. An-\\nother major issue is the red herring fallacy or the\\nover-association of unrelated facts. Addressing this\\nchallenge calls for high-level sensemaking rather\\nthan mere fact-level verification.Interest Level', 'source': 'data/storm_paper.pdf'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id=166, version=0, score=0.6739614, payload={'content': 'While articles produced by STORM are preferred\\nby both automatic metrics and human evaluation,\\nexperienced editors still identified multiple prob-\\nlems with the machine-generated articles. We an-\\nalyze the free-form comments and summarize the\\nmajor issues in Table 11.\\nThe primary issue raised is that the generated\\narticles often contain emotional language and lack\\nneutrality, primarily due to the source material.\\nSTORM currently retrieves grounding sources', 'source': 'data/storm_paper.pdf'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id=34, version=0, score=0.6735942, payload={'content': 'the question asking process. Specifically, STORM\\nprompts an LLM to generate a list of related top-\\nics and subsequently extracts the tables of contents\\nfrom their corresponding Wikipedia articles, if such\\narticles can be obtained through Wikipedia API7\\n(Figure 2 1). These tables of contents are con-\\ncatenated to create a context to prompt the LLM\\nto identify Nperspectives P={p1, ..., p N}that\\n7https://pypi.org/project/Wikipedia-API/can collectively contribute to a comprehensive ar-', 'source': 'data/storm_paper.pdf'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is storm framework\"\n",
    "results = search(question, top_k=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b1eaca-583c-4560-a707-94fecfb1eebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In §3, we introduce STORM, a framework that au-\\ntomates the pre-writing stage by discovering differ-\\nent perspectives, simulating information-seeking\\nconversations, and creating a comprehensive out-\\nline. Algorithm 1 displays the skeleton of STORM.\\nWe implement STORM with zero-shot prompt-\\ning using the DSPy framework (Khattab et al.,\\n2023). Listing 1 and 2 show the prompts used\\nin our implementation. We highlight that STORM\\noffers a general framework designed to assist the'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[130]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ee5d5-f074-42ce-9f11-7632a0f5e5e6",
   "metadata": {},
   "source": [
    "## Retrieval using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d047d99-968e-4527-a987-38d9bebef748",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41befe04-b03f-49f7-9cb7-eef05ba2a705",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9db151fa-cc89-40d8-a29c-e41a0e4ba6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an assistant for question-answering tasks. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "references = [obj.payload[\"content\"] for obj in results]\n",
    "\n",
    "\n",
    "context = \"\\n\\n\".join(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e8ac805-9a62-417f-8ddf-806fd25b7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "  api_key=OPENAI_API_KEY,\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"content\": system_prompt.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc67fd58-6d67-45da-9d87-1c19040e611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM is a framework that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating a comprehensive outline. It is implemented using zero-shot prompting with the DSPy framework and is designed to assist in the article writing process.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c85f3-ebb5-4267-903a-4abf114a0d7f",
   "metadata": {},
   "source": [
    "## Response with References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39eca50e-fdbc-45cb-bc95-bae0157a75f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: STORM is a framework introduced in the text that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating a comprehensive outline. It is implemented with zero-shot prompting using the DSPy framework and offers a general framework designed to assist in the question-asking process.\n",
      "\n",
      "\n",
      "REFERENCES:\n",
      "\n",
      "Reference: [1]: In §3, we introduce STORM, a framework that au-\n",
      "tomates the pre-writing stage by discovering differ-\n",
      "ent perspectives, simulating information-seeking\n",
      "conversations, and creating a comprehensive out-\n",
      "line. Algorithm 1 displays the skeleton of STORM.\n",
      "We implement STORM with zero-shot prompt-\n",
      "ing using the DSPy framework (Khattab et al.,\n",
      "2023). Listing 1 and 2 show the prompts used\n",
      "in our implementation. We highlight that STORM\n",
      "offers a general framework designed to assist the\n",
      "\n",
      "Reference: [2]: 50$ for our study.for each article. Figure 7 shows the screenshot of\n",
      "our web application and the full article produced\n",
      "by STORM is included in Table 12. For human\n",
      "evaluation, we use a 1 to 7 scale for more fine-\n",
      "grained evaluation. The grading rubric is included\n",
      "in Table 10.\n",
      "We collected the pairwise preferences and the\n",
      "perceived usefulness of STORM via an online ques-\n",
      "tionnaire. Specifically, for the perceived usefulness,\n",
      "we request editors to rate their agreement with state-\n",
      "\n",
      "Reference: [3]: STORM currently retrieves grounding sources\n",
      "from the Internet which is not neutral and con-\n",
      "tains considerable promotional content on its own.\n",
      "Addressing this bias in the pre-writing stage repre-\n",
      "sents a valuable direction for future research. An-\n",
      "other major issue is the red herring fallacy or the\n",
      "over-association of unrelated facts. Addressing this\n",
      "challenge calls for high-level sensemaking rather\n",
      "than mere fact-level verification.Interest Level\n",
      "\n",
      "Reference: [4]: While articles produced by STORM are preferred\n",
      "by both automatic metrics and human evaluation,\n",
      "experienced editors still identified multiple prob-\n",
      "lems with the machine-generated articles. We an-\n",
      "alyze the free-form comments and summarize the\n",
      "major issues in Table 11.\n",
      "The primary issue raised is that the generated\n",
      "articles often contain emotional language and lack\n",
      "neutrality, primarily due to the source material.\n",
      "STORM currently retrieves grounding sources\n",
      "\n",
      "Reference: [5]: the question asking process. Specifically, STORM\n",
      "prompts an LLM to generate a list of related top-\n",
      "ics and subsequently extracts the tables of contents\n",
      "from their corresponding Wikipedia articles, if such\n",
      "articles can be obtained through Wikipedia API7\n",
      "(Figure 2 1). These tables of contents are con-\n",
      "catenated to create a context to prompt the LLM\n",
      "to identify Nperspectives P={p1, ..., p N}that\n",
      "7https://pypi.org/project/Wikipedia-API/can collectively contribute to a comprehensive ar-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"ANSWER: {response.choices[0].message.content}\\n\\n\")\n",
    "print(f\"REFERENCES:\\n\")\n",
    "for index, ref in enumerate(references):\n",
    "    print(f\"Reference: [{index + 1}]: {ref}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24603d-6d21-46c9-a308-2f317e2e08e9",
   "metadata": {},
   "source": [
    "## Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5669ffd6-91c3-4c40-b370-999e89901da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM is a framework introduced in the context that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating a comprehensive outline. It is implemented with zero-shot prompting using the DSPy framework and is designed to assist in the question-asking process.None"
     ]
    }
   ],
   "source": [
    "response = completion(\n",
    "  api_key=OPENAI_API_KEY,\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{ \"content\": system_prompt.format(context=context),\"role\": \"system\"}, { \"content\": user_prompt.format(question=question),\"role\": \"user\"}],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64892f-93f0-4886-96c0-16bbaca823a2",
   "metadata": {},
   "source": [
    "## Use Local models via Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2959d644-a580-4663-9415-47456e45b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The STORM framework is an automated pre-writing stage discovery tool that simulates information-seeking conversations and creates a comprehensive outline, as introduced in §3 of the given context."
     ]
    }
   ],
   "source": [
    "response = completion(\n",
    "  model=\"ollama/llama3\",\n",
    "  messages=[{\"content\": system_prompt.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}],\n",
    "  api_base=\"http://localhost:11434\",\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cd984-4bb9-4ae7-9d86-41bb0aee8d14",
   "metadata": {},
   "source": [
    "## Response Generation using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daae89-81fb-429d-adc3-6a480ea110a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is storm?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a7987-2748-495e-b054-3318c76ceccc",
   "metadata": {},
   "source": [
    "## Langchain and Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b83998-61bf-42d3-9717-8ee4754f74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32aac0-f57f-4a3b-b4aa-eaad7aa120fa",
   "metadata": {},
   "source": [
    "## Advanced RAG Topics\n",
    "\n",
    "- Query routing\n",
    "- Multi-document queries\n",
    "- Multi-modal queries\n",
    "- etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
